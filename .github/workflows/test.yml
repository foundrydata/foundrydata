name: Testing v2.1 Matrix

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ "**" ]

concurrency:
  group: tests-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

env:
  # Global defaults for Testing v2.1 (can be overridden per job)
  TEST_SEED: "424242"
  DEBUG: "false"

jobs:
  lint:
    name: ESLint (zero errors policy)
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20.x
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: ESLint check
        id: eslint_check
        run: npm run lint
        continue-on-error: true

      - name: Auto-fix (no commit) and prepare patch
        if: steps.eslint_check.outcome == 'failure'
        run: |
          npm run lint:fix || true
          git diff > eslint-fixes.patch || true
          echo "ESLint found issues. Auto-fix attempted. See artifact patch."

      - name: Upload ESLint fixes patch
        if: steps.eslint_check.outcome == 'failure'
        uses: actions/upload-artifact@v4
        with:
          name: eslint-fixes-patch
          path: eslint-fixes.patch
          if-no-files-found: ignore

      - name: Fail if ESLint errors remain
        if: steps.eslint_check.outcome == 'failure'
        run: |
          echo "ESLint errors remain after auto-fix. Failing policy." >&2
          exit 1

  quick-tests:
    name: Quick Tests (${% raw %}{{ matrix.os }} 路 Node {{ matrix.node }} 路 {{ matrix.draft }}{% endraw %})
    needs: [lint]
    runs-on: ${{ matrix.os }}
    timeout-minutes: 5
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        node: [18.x, 20.x, 22.x]
        draft: [draft-07, 2019-09, 2020-12]
    env:
      CI: "true"
      SCHEMA_DRAFT: ${{ matrix.draft }}
      FC_NUM_RUNS: "100"
      # Test pool configured in vitest config (threads on Windows, forks otherwise)
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node }}
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Run quick tests (with annotations)
        run: |
          echo "Running quick tests with SCHEMA_DRAFT=${SCHEMA_DRAFT}, FC_NUM_RUNS=${FC_NUM_RUNS}"
          npx vitest run --config vitest.config.ts --reporter=default --reporter=github-actions

      - name: Upload raw test logs (optional)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: quick-tests-logs-${{ matrix.os }}-node-${{ matrix.node }}-draft-${{ matrix.draft }}
          path: |
            ./**/vitest*.log
          if-no-files-found: ignore

  full-tests:
    name: Full Tests (${% raw %}{{ matrix.os }} 路 Node {{ matrix.node }} 路 {{ matrix.draft }}{% endraw %})
    needs: [quick-tests]
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        node: [18.x, 20.x, 22.x]
        draft: [draft-07, 2019-09, 2020-12]
    env:
      CI: "true"
      SCHEMA_DRAFT: ${{ matrix.draft }}
      FC_NUM_RUNS: "1000"
      PERF_LOG: "false"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node }}
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Run full tests with coverage + annotations
        run: npx vitest run --config vitest.config.ts --coverage --reporter=default --reporter=github-actions

      - name: Enforce coverage threshold (90% lines)
        run: |
          node -e "const s=require('./coverage/coverage-summary.json'); const pct=s.total.lines.pct; if (pct<90){console.error(\`Coverage lines ${pct}% < 90%\`); process.exit(1);} else {console.log(\`Coverage lines ${pct}% >= 90%\`);}"

      - name: Upload coverage (lcov + summary)
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.os }}-node-${{ matrix.node }}-draft-${{ matrix.draft }}
          path: |
            coverage/lcov.info
            coverage/coverage-summary.json
            coverage/index.html
          if-no-files-found: ignore

      - name: Attempt JUnit export (best-effort)
        run: |
          mkdir -p test-results || true
          npx vitest run --config vitest.config.ts --reporter=junit --outputFile test-results/junit.xml || echo "JUnit reporter not available; skipping."
        continue-on-error: true

      - name: Upload JUnit results
        if: hashFiles('test-results/junit.xml') != ''
        uses: actions/upload-artifact@v4
        with:
          name: junit-${{ matrix.os }}-node-${{ matrix.node }}-draft-${{ matrix.draft }}
          path: test-results/junit.xml

  performance-tests:
    name: Performance & Baseline
    needs: [quick-tests]
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      CI: "true"
      SCHEMA_DRAFT: "2020-12"
      FC_NUM_RUNS: "1000"
      PERF_LOG: "true"
      NODE_OPTIONS: "--expose-gc"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20.x
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Preserve previous baseline (if any)
        run: |
          if [ -f test/performance/baseline.json ]; then
            cp test/performance/baseline.json test/performance/baseline.prev.json
          fi

      - name: Run performance suites
        run: |
          npm run test:performance
          npm run test:benchmarks
          npm run test:regression

      - name: Check performance regression (>20% p95 or >100MB memory)
        id: perf
        shell: bash
        run: |
          node <<'NODE'
          const fs = require('fs');
          const path = require('path');
          const prevPath = path.join('test','performance','baseline.prev.json');
          const currPath = path.join('test','performance','baseline.json');
          let regressions = [];
          if (fs.existsSync(prevPath) && fs.existsSync(currPath)) {
            const prev = JSON.parse(fs.readFileSync(prevPath,'utf8'));
            const curr = JSON.parse(fs.readFileSync(currPath,'utf8'));
            for (const name of Object.keys(curr.benchmarks||{})) {
              const c = curr.benchmarks[name];
              const p = prev.benchmarks ? prev.benchmarks[name] : undefined;
              if (!p) continue;
              const p95reg = ((c.percentiles.p95 - p.percentiles.p95) / p.percentiles.p95) * 100;
              const memReg = (c.memory && p.memory) ? ((c.memory.delta - p.memory.delta) / (p.memory.delta || 1)) * 100 : 0;
              const memAbs = (c.memory && p.memory) ? (c.memory.delta - p.memory.delta) : 0;
              if (p95reg > 20) {
                regressions.push({ name, metric: 'p95', change: p95reg, baseline: p.percentiles.p95, current: c.percentiles.p95 });
              }
              if (memAbs > 100 * 1024 * 1024) {
                regressions.push({ name, metric: 'memory(+MB)', change: memAbs/1024/1024, baseline: p.memory?.delta, current: c.memory?.delta });
              }
            }
          }
          const report = ['# Performance Report', ''];
          if (regressions.length === 0) {
            report.push('No regressions detected vs previous baseline.');
          } else {
            report.push('## Regressions Detected');
            for (const r of regressions) {
              report.push(`- ${r.name}: ${r.metric} regression: +${r.change.toFixed ? r.change.toFixed(1) : r.change} ${r.metric==='p95'?'%':'MB'}`);
            }
          }
          fs.mkdirSync('test/performance', { recursive: true });
          fs.writeFileSync('test/performance/performance-report.md', report.join('\n'));
          const has = regressions.length > 0;
          fs.appendFileSync(process.env.GITHUB_OUTPUT, `has_regression=${has}\n`);
          if (has) {
            console.error('Performance regressions detected. See performance-report.md');
          }
          process.exit(has ? 1 : 0);
          NODE

      - name: Upload performance artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-artifacts
          path: |
            test/performance/baseline.json
            test/performance/baseline.prev.json
            test/performance/performance-report.md
            test/performance/regression-cases.json
          if-no-files-found: ignore

      - name: Comment on PR with regression summary
        if: ${{ failure() && github.event_name == 'pull_request' }}
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = 'test/performance/performance-report.md';
            const body = fs.existsSync(path) ? fs.readFileSync(path, 'utf8') : 'Performance regressions detected.';
            github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body
            });

  memory-tests:
    name: Memory Tests (leak + GC)
    needs: [quick-tests]
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      CI: "true"
      SCHEMA_DRAFT: "2020-12"
      FC_NUM_RUNS: "1000"
      NODE_OPTIONS: "--expose-gc"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20.x
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Run memory/load test suite
        run: |
          npx vitest run test/__tests__/integration/performance/memory-load.test.ts --config vitest.config.ts

      - name: Upload memory test logs (optional)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: memory-test-logs
          path: ./**/vitest*.log
          if-no-files-found: ignore
